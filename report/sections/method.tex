\chapter{Method}
This chapter describes the different steps that were required to create the automatic package measuring algorithm.
First an overview of the system is presented, followed by a more in-depth description.
Then, the method used to evaluate the system is presented.
Finally, a brief overview of the used software and hardware is given.

\section{System description} \label{method:implementation}
This section begins with an overview of the system, followed by a more in-depth description of the components.

\subsection{Overview}
This section provides an overview of the different components that were necessary to measure packages automatically.
A flowchart which describes the process is shown in figure \ref{fig:flowchart}.
On the topmost level in the flowchart are the two components which have been described earlier: detection and measuring. 
The detection component uses an image as input, and outputs the positions of the corners of the reference object and package.
The measuring component takes the corner positions as input, and outputs the dimensions of the package.

The different steps of the detection component and their respective purpose are summarised below:
\begin{itemize}
	\item \textbf{Preprocessing.} In the preprocessing step, the image is blurred with a median filter and converted from colour to grayscale. Its purpose is to facilitate the subsequent edge detection step by reducing noise and removing unnecessary details from the image. This step is described further in section \ref{method:image_processing}.
	\item \textbf{Edge detection.} As the name implies, the goal of the edge detection step is to detect edges in the image. This is done with Canny's edge detection algorithm \cite{canny}, which is described in section \ref{method:edge_detection}.  Edge detection provides a way to extract the important features of the image, which can be interpreted at later stages.
	\item \textbf{Line detection.} In this stage, lines are detected based on the edge data from the previous step. This is done with the Hough transform \cite{illingworth1988survey}, which is described in section \ref{method:line_detection}. The purpose of this step is to convert the edge data, which is received as a binary edge image, to a symbolic format, i.e. the end points of detected lines in the image. 
	\item \textbf{Package detection.} This step uses the line data and the location of the reference object to detect the package in the image. A simple model-based algorithm is used, which uses the fact that the outline of a cuboid package consists of three pairs of parallel lines. The sought information is the positions of the corners of the package. This algorithm is described in section \ref{method:package_detection}.
 	\item \textbf{Reference object detection.} The reference object is detected using a modified version of the package detection algorithm. It is described in section \ref{method:reference_object_detection}.
\end{itemize}
After detection, the measuring takes place, using the detected corner points. This algorithm is based on the theory presented in chapter \ref{chapter:projective_geometry} and it is described in section \ref{fig:screenshot} The steps are summarised below:
\begin{itemize}
	\item \textbf{Estimate a planar homography.} A planar homography is calculated between the image plane and the top plane of the package, using the known location and size of the reference object.
	\item \textbf{Calculate world coordinates of top corners.} Using the homography, the world coordinates of the top corners of the package can be calculated, since they lie in the same plane as the reference object.
	\item \textbf{Calculate length and width.} The length and width of the package are calculated with the euclidean distance between the top corners.
	\item \textbf{Estimate vanishing points.} Vanishing points are estimated using the intersections between edges of the package as shown in figure \ref{fig:vanishing_points}.
	\item \textbf{Estimate intrinsic parameters.} The intrinsic parameters are calculated by imposing constraints on the image of the absolute conic as described in section \ref{background:vanishing_point_calibration}. The vanishing points and the planar homography are used to do this.
	\item \textbf{Estimate camera pose.} Using the intrinsic parameters and the planar homography, the camera pose is estimated, as described in section \ref{camera-pose}.
	\item \textbf{Calculate height.} Finally, when the intrinsic matrix $K$ and the extrinsic matrix $[R|t]$ are known, the height can be calculated using an equation system derived from equation \ref{eq:projection3}. This is made possible by the fact that a top corner shares $X$ and $Y$ coordinates with each bottom corner. 
\end{itemize}

\begin{figure}
\begin{center}
\includegraphics[width=1.0\textwidth]{figures/flowchart.png}
\end{center}
\caption[A flowchart illustrating the workflow of the program]{A flowchart which illustrates the workflow of the program.}
\label{fig:flowchart}
\end{figure}

\subsection{Preprocessing} \label{method:image_processing}
%The primary goal in image processing is to subjectively improve the quality of an image. 
%The image is typically a digital image, which consists of small image elements, pixels.
Preprocessing is often the first step in computer vision systems.
It consists of for example applying filters to improve the quality of the image for the specific task at hand.
%Running image processing algorithms are often the first steps in computer vision systems, which is often referred to as preprocessing.
%Both the input and output of an image processing algorithm are typically images, for example algorithms for noise reduction, contrast enhancement, and colour correction \cite[p. 1-2]{gonzalez-woods}. 

%As the purpose is subjective improvement of quality, the means of performing preprocessing varies greatly depending on the application.
In this case, the aim is to reduce noise, and to remove irrelevant information from the image.
This is accomplished by converting the colour image to a grayscale image, followed by applying a median filter. 
A median filter was chosen because it reduces noise effectively, while also removing small, unwanted features from the image, such as small labels and marks on the package or the rest of the scene.
At the same time edges are preserved.
As the name implies, the median filter assigns the value of every pixel to the median of itself, and the neighbouring pixels within a certain radius \cite{huang1979fast}.

\subsection{Edge detection} \label{method:edge_detection}
After preprocessing, the next step was to extract features from the image which can be used to make an interpretation of the image.
More specifically, the features that were extracted are edges. 
This was done using Canny's edge detection algorithm \cite{canny}. % TODO some additional comment maybe?
The outline of Canny's algorithm is as follows:
\begin{enumerate}
	\item Apply a smoothing image filter to reduce noise. A Gaussian filter is typically used, but a median filter was found to work better for this application.
	\item Find the intensity gradient of the image, i.e. the first derivative of the intensity along the x-axis and y-axis combined. The result is an image which is bright where the intensity changes rapidly, i.e. there is an edge.
	\item Apply non-maximum suppression, which is an edge thinning technique. The gradient image can often be blurry and contain thick edges. Non-maximum suppression on only keeps the local maxima, and sets the rest of the values to 0.
	\item Hysteresis, to remove false edges. At this point, the intensity image will contain many weak edges which are irrelevant. To remedy this, two thresholds are used, a low threshold and a high threshold. Pixels in the gradient image with a value lower than the low threshold are rejected, and pixels with a value  higher than the high threshold are accepted. Pixels with values in between the two thresholds are accepted if they are connected to a pixel above the high threshold. Accepted pixels are set to 1, and rejected pixels to 0, resulting in an accurate, binary edge image.
\end{enumerate}

% TODO skriva något om contour analysis?  \cite{suzuki} 
% TODO do this!!! ^--- very important pruning step
\subsection{Line detection} \label{method:line_detection}%TODO tempus korrekt? % TODO add image? of sinusoids and each step.
Since the package consists of a set of lines, and the edge image is difficult to interpret as it is, it seems like a good idea to find the lines in the image.
This was done with the Hough transform \cite{illingworth1988survey}.
The Hough transform can be used to detect many shapes.
The idea behind the Hough transform for line detection is as follows:
\begin{enumerate}
	\item In polar coordinates, $(r, \theta)$, a line is represented as: $$y = -\frac{\cos \theta}{\sin \theta} x + \frac{r}{\sin \theta}.$$ This can be rewritten as: $r = x \cos \theta + y \sin \theta$. 
	\item Every possible line passing through the point $(x,y)$ forms a sinusoid in the $\theta - r$ plane. We can calculate the sinusoid for every point of interest (each edge pixel).
	\item The sinusoids of two image points will intersect in the $\theta - r$ plane, in a point which represents a line between the two image points. If multiple sinusoids intersect in the same point, the same number of image points lie in a line.
	\item The more sinusoids intersect at a point $p$ in the $\theta - r$ plane, the more likely it is that the line represented by $p$ is truly a line in the image. Given a threshold $t$, the Hough transform declares that there is a defined line by $(r, \theta)$ in the image, if the number of intersections in the is $\theta - r$ plane greater than $t$.
\end{enumerate}
The above is the principle behind the Hough transform.
In practice, a more efficient, probabilistic version presented in \cite{houghp} was used. 
This algorithm yields the end points of detected line segments in the image.
That is better than the edge image, since some irrelevant edges are removed, and the representation is more convenient.

\subsection{Package detection} \label{method:package_detection}
The algorithm used to detect the package was based on a ranking scheme.
Multiple hypothesis about the location of the package were created, and then scored. 
The candidate with the highest score was chosen as the assumed package. 

A fundamental property of cuboids was exploited by the algorithm: they consist of parallel line segments of equal length. 
More specifically, the outline of a cuboid in an image is made up of three pairs of parallel line segments, which form a hexagon. 
However because of perspective distortion the line segments are not entirely parallel, and not of exactly equal length (probably).
This is where the ranking algorithm came in.
The assumption behind the ranking was that the package was the hexagon with the least error in parallelism and length difference, in the image.
Thus, the ranking algorithm rated a package candidate according to the degree of parallelism and equality of lengths between opposing sides.

First however, the candidates had to be found.
This was done with the following steps:
\begin{enumerate}
	\item Find all pairs of parallel line segments. 
		As stated earlier, the line segments are unlikely to be exactly parallel, due to perspective distortion.
		Thus, a pair of line segments are considered to be parallel if the angle between them is below a threshold.
		This threshold is set to be rather high, $30^\circ$. 
		That is to avoid discarding a pair that belongs to the actual package, since the effect of perspective distortion is large from some perspectives. % TODO be more specific? example? max distortion?
	\item For every three pairs of parallel line segments, let their intersections form a polygon.
		The sought intersections are those between the lines defined by the line segments, not the line segments themselves.
		This is because the full contour of the package is not always distinguishable.
		The polygon is considered to be a candidate if it passes some basic tests.
		For example, it must be convex, have six corners, and neighbouring sides in the polygon cannot belong to the same line pair. 
		The corner must also be within the bounds of the image.
		Additionally, if the location of the reference object is known, the package must enclose the reference object.
\end{enumerate}

When all candidates had been generated they were scored according to the rules above.

This algorithm is quadratic in the total number of detected lines and cubic in the number of parallel line pairs.
In some cases, that was too slow, especially on a mobile device with very limited computation time.
If there was too much data the algorithm will therefore abort the detection at this stage.
Specifically, 100 parallel line pairs was declared as the maximum.
To reduce the risk of having to discard an image, some pruning was attempted at different stages in the algorithm.
For example, edges at the very outskirts of the image were removed, since it was assumed that the package was somewhat in the centre of the image.
Edges that are too small were also removed.
In the package detection step itself, pruning was done by for example demanding that the distance between parallel line pairs was above a threshold, i.e. a minimum size was enforced on the package.

\subsection{Reference object detection} \label{method:reference_object_detection}
The reference object was detected using a modified version of the package detection algorithm.
Since the reference object was a paper, which is a quadrilateral, two pairs of parallel line segments were picked out at a time to form a quadrilateral.
The ranking algorithm also used some additional criteria, since the reference object is a white ISO 216 A-series  paper, the score was also based on how bright the area within the polygon was, and how well the aspect ratio between the two sides matched that of a ISO 216 A-series paper.

\subsection{Measuring} \label{method:measuring} % TODO describe 2 face vs 3 face recognition
The measuring was carried out in two steps, first the width and depth of the package were measured, followed by the height. 
The reason behind that order was that the reference object was placed on top of the package, meaning that some extra information was known about the  top plane of the package.

As described in \ref{planar-homographies} the world coordinate system can be defined to have its origin in one of the corners of the reference object.
Since the physical size of the reference object is known, the position of the corners of the reference object is known in both image and world coordinates.
With the four point correspondences, a planar homography could be calculated, which could then be used to map any point in the image to its corresponding point in the plane.
Using the homography, the positions of the four top corners of the package could then be calculated. 
Finally, the width and the depth were calculated by calculating the euclidean distance between the world coordinates of the corners.

To calculate the height, the world coordinates of one of the lower corners of the package were still needed.
This is why camera calibration is useful, because when the intrinsic and extrinsic parameters are known, equation \ref{eq:projection1} can be used.

As stated earlier, two types of calibrations were to be implemented, one using an offline configuration which must be done beforehand, and one using online calibration.
The offline version was implemented with Zhang's method and the online version was implemented with vanishing point calibration.

The offline calibration was performed once.
As stated in section \ref{camera-calibration}, the procedure is to print out a paper with a known pattern (a checkerboard pattern), take images of the paper from many positions, and feed the images to the algorithm.

Vanishing point calibration is ideal for this particular problem, since every cuboid package has three orthogonal sets of parallel lines which result in three orthogonal vanishing points, as in figure \ref{fig:vanishing_points}.
Additionally, the known homography between the image plane and the top-plane of the package can be used to improve the estimate of $K$.

When $K$ had been obtained by either calibration method, the camera pose was estimated as described in \ref{camera-pose}, using the four image-world point correspondences given by the reference object. % TODO which method?
%The method EPnP was chosen initially, but much better results were achieved with a regular, iterative approach.

Once the camera pose was obtained, equation \ref{eq:projection1} could be used, meaning that any point in the world can be projected to a point in the image.
However, the reverse mapping was sought, the position of a bottom corner of the package in world coordinates, given its position in image coordinates.
This is not possible to do, unless something is known about the geometry of the scene.
Luckily, it is known that each lower corner shares $X$, and $Y$ coordinates with a top corner, which position is already known.
The image coordinates $(u,v)$ of a lower corner, and the world coordinates $(X,Y)$ of the corresponding upper corner could then be used to solve for the $Z$ coordinate of the lower corner. 
Equation \ref{eq:projection3} was transformed to the following over-determined system:
\begin{equation} \label{eq:constrained-projection}
\begin{pmatrix} up_{33}-p_{13} \\ vp_{33}-p_{23} \end{pmatrix} Z = 
\begin{pmatrix}
X(p_{11}-up_{31}) + Y(p_{12}-up_{32})+p_{14}-u \\
X(p_{21}-vp_{31}) + Y(p_{22}-vp_{32})+p_{24}-v
\end{pmatrix}.
\end{equation}
This was solved as a least-squares problem using Single Value Decomposition (SVD).
Since two pairs of upper-lower corners which share $X$ and $Y$ coordinates were known, two least-squares solutions of $Z$ could be obtained.
The solution with the smallest error was chosen as $Z$.

\section{Evaluation} \label{method:evaluation}
To evaluate the performance of the method, a test dataset of 225 images of packages were taken.
Five different packages were in the dataset, with 45 images of each.
The packages in the test dataset are shown in figure \ref{fig:packages}. A blue plastic folder was placed under the reference object on packages 3-5 to add some contrast between the reference object and the white surfaces.
The images were taken with a Samsung Galaxy Edge S6 at 16 megapixels ($2988 \times 5312$ pixels) and then scaled down to $720 \times 1280$ pixels, to reduce computational time. % TODO ok att ha olika upplösning på offline och online?
\input{floats/packages}

The test dataset was constructed systematically with images observing the packages from different positions.
Three parameters were alternated: 
\begin{itemize}
	\item Distance to the centre of the package. 
			The used distances were 1, 1.5, and 2 metres.
	\item Height above the floor. 
			The used heights were 0.9, 1.2, and 1.5 metres for the first three packages, and 1.2, 1.5, and 1.8 metres for the latter two.
			The reason for the division is that the high height of the two latter packages made the viewing angle very small when viewing them from 0.9 metres above ground. The lowest height was removed for these packages and a higher one added.
	\item Rotation. 
			Five different angles were used.
			Two were frontal views of the package, in which only two faces of the package could be observed. One of the longer side, and one of the shorter side.
			One image was taken between the two sides, with a $45^\circ$ angle to both sides.
			The last two images view the package with a sharp angle, about $15^\circ$ to the long or short side, respectively.
			An example from each position is shown in figure \ref{fig:positions}.
\end{itemize}

The distances 1, 1.5 and 2 metres were chosen because 1 and 2 metres were thought to be a lower and upper bound for distances that most users would naturally use, and 1.5 metres is in between the two.
The heights were chosen by the same logic.

\input{floats/angles}

\subsection{Benchmarking} \label{benchmarking}
A benchmarking program was made in order to measure the performance of the algorithm on the test dataset.
The detection algorithm and measuring algorithm were tested both in isolation, and combined, to show the performance of the individual pieces and of the whole system. 

The detection algorithm was tested by comparing the resulting corner points against ground truth positions of the reference object and package corners, which had been extracted manually in each image.

The measuring algorithm was tested by using the ground truth positions as input, and comparing the resulting measurements against the manually measured ground truth dimensions of the package.
Measurements were made with both calibration methods:, online vanishing point calibration, and offline calibration with Zhang's method. 

Finally, the overall ability of the system was tested by running the whole algorithm and comparing the result with the ground truth dimensions of the package. 

\subsection{Online testing} \label{method:online_testing}
A simple proof of concept application for Android was made for online testing on a smartphone.
This was used to assert that it is feasible to run the algorithm on a smartphone, and to determine if it was possible to run the algorithm in real-time.

The application simply displays the camera feed and continuously feeds new frames to the measuring algorithm.
When the package or reference object is successfully detected, an overlay is drawn to outline their position on the screen.
If both the package and reference are found in the same frame, the dimensions of the package are calculated, and each dimension is drawn next to the appropriate edge.
The processing time for each frame is also displayed.
A screenshot of the application is shown in figure \ref{fig:screenshot}.

\section{Implementation details}
This section gives an overview of the most important software and hardware which was used during implementation and testing.

\subsection{OpenCV} % TODO beskriv vad jag har använt ur opencv?
The most important software used in this thesis is the library OpenCV.
It is used extensively in this thesis, and therefore deserves an introduction.

OpenCV is an open source library for computer vision.
It contains over 2500 optimised, state-of-the-art computer vision algorithms \cite{opencv_about}.
The functions in OpenCV range from low-level image filters to high-level algorithms for object recognition, camera calibration, pose estimation, machine learning, and much more.

OpenCV has a large community backing it, consisting of both startup companies, large corporations and researchers.
Development is led by the Russian company Itseez, and among the supporters are Google, Microsoft, Intel, IBM and Toyota \cite{opencv_about}. 

OpenCV is written in C++, but also has full interfaces for Python, Java and MATLAB and supports a variety of platforms, including Windows OS X, Linux, Android and iOS \cite{opencv_about}.
The native C++ interface was used in this thesis.

\subsection{Hardware}
The device used to run the test application was a Samsung Galaxy S6 Edge.
The S6 Edge was chosen because it currently tops benchmarks and tests, both regarding processing power and photo quality \cite{phone_performance_benchmark}\cite{phone_camera_benchmark}.
The S6 Edge was also used to capture the test images used for offline testing.
