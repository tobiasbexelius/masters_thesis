\chapter{Discussion}
This chapter discusses the strengths and weaknesses of the used method, limitations of the results, and proposes ways to improve upon this work.

\section{Strengths}
The results showed that the used method works well when using images of a package being observed from an angle where three faces of the package are visible, and at sufficiently close range and high altitude. 
As shown in table \ref{table:measurement_overall}, the success rate increased from 50\% to 92\% when selecting a subset of the camera poses.
The average error was also reduced from 8\% to 7\%.
The results from the selected subset of camera poses are satisfying, and suggestions on how to improve the performance of the remaining cases are proposed in \ref{discussion:weaknesses}

Aside from success rates, this method performs the measurements of cuboids automatically, entirely without user intervention.
Additionally, no prior calibration, or special calibration objects are required.
That is avoided by using the fact that the package acts as a calibration object.
The only additional required object is a reference object.
In the current implementation, only white papers are detected, but detection of other planar objects, like credit cards for example, can easily be added.
A different planar reference object can be used directly by the measuring algorithm, the only requirement is that it must fit on top of the package.
With a small modification it could be placed on the floor next to the package instead.

% TODO calculate good positions success rate for measuring based on key points as well, so that the performance of the different components can be discussed.
% TODO perhaps present errors per dimension? Discuss error here. % 

\section{Weaknesses} \label{discussion:weaknesses}
A few different cases where the detection and measuring algorithms fail were observed through testing.

\subsection{Detection}
The detection algorithms were based on simple, feature-based method. 
The method involved detecting lines in the image and attempting to match the intersections between lines to a quadrilateral or hexagon in which opposing sides have as equal length, and are as parallel as possible.

The image processing algorithms used by this method depend on several constants.
Finding good values for the constants is critical.
They can not be too strict, because then important features can sometimes be left out, but they can not be too tolerant either, because then too much unimportant data can be left in the image, so that it is difficult to make sense of the data, or it takes too long to process.
For this task, is seemed impossible to choose a single value for a constant which yielded good enough data every time.
Many examples where there was too much, or too little data left to analyse for the paper or package detection algorithms exist in the test dataset.
This balance was extra important in this case, since the algorithm is supposed to be run on a smartphone with with limited processing power and with limited processing time.
This was one of the most common reasons of failure for the detection algorithms.

Another problem is that the rating criteria were not sophisticated enough.
In some cases there was another hexagon with opposing edges more parallel and with more similar length than the package.
This occurred more often for two-face than the three-face views.

Some other criteria were considered, but sadly there was not enough time to implement them.
For example additional score could be awarded if the inner contours of the package are detected.
Currently, the inner contours of the package are filtered out, to reduce the amount of data.
This addition would require them to be kept, which would in turn call for more intelligent pruning of edges as packages sometimes have a lot of texture which can make the amount of data overwhelming.

Another reason for the poor performance of two-face views is that as distance grows, the edges going from bottom to the middle and from the middle to the top of the package contour, become increasingly parallel. 
If the picture is not taken precisely in front of the middle of the package, but slightly towards the right or the left, the two edges can become inseparable.

% TODO refer more to results <<<<<<<<<<<<<<<
% also room for much irrelevant stuff when far away, and things behind on the wall (!!!!)

% TODO additional reasons:
% from some angles the constraints imposed by the detection algorithm causes detection to fail (?)
% sometimes packages fail because they are not straight enough (?)

%In the current implementation only a tiny bit of an edge must be detected for it to be used as a potential package edge, since it is often the case that the full package contour cannot be found.
%It is not modelled that a candidate with detected edges along much of its contour is more likely to be the package.
%This could help with nonsensical candidate sometimes winning.

\subsection{Measuring}
The results in section \ref{results:measuring} showed that vanishing point calibration result fail more often than when using Zhang's method, but the accuracy is almost the same between the two methods. (TODO: methods only differ for the z dimension, though..) % TODO !!
Two main reasons have been found to cause the lower success rate of vanishing point calibration: infinite vanishing points, and increased vulnerability to errors in detection.

Infinite vanishing points occur when the parallel lines of the package are parallel in the image as well, and hence do not result in a finite vanishing point. 
If an infinite vanishing point does occur, a constraint on $\omega$ is lost
However, since the system is overdetermined, the calibration should still work in theory, even with up to two infinite vanishing points, if the vanishing point in the $Z$-direction is not one of the infinite ones, in which case the calibration becomes degenerate.
In practice the error was often too great, when one vanishing point was infinite. % TODO find out how many times this occurred in test set and if it worked with an infinite vp any of the cases

The vanishing point calibration method is also more vulnerable to errors in the package corners, since both the calibration and measurements themselves depend on the location of the package corners being correct.
%Errors do not necessarily increase proportionally with the error 
% TODO find out correlation between error in detection and measurement! 

%Due to the inaccuracies a different solution than the correct one have smaller least squares error than the correct answer, is therefore chosen as the solution.
%These problem occur when using detected corner points as well as when using the marked corners, since the marked corners also have some error.
%On average, the accepted solutions have the same error when using detected corner points and marked corners. % TODO förtydliga hur marked corners går till i metod

\section{Online testing}
Realtime vs not realtime. Processing time. % TODO make formal measurements of processing time, should be very easy to calc average

\section{Limitations}
Because of limitations in the evaluation method, it should be notedThere are some limitations as to what the results can show, as a consequence of limitations in the evaluation method.
For example only one room was used for testing.
It was not an optimal environment, due to the striped floor and suboptimal lightning conditions.
The colour of the floor was also similar to the colour of one of the package.
However, performance is not guaranteed to be the same in other environments.

Another limitation is that not much effort was put into dealing with false positives.
The only countermeasure to this is that minimum score thresholds where added to the rating algorithm, i.e. a reference object or package hypothesis with a too low score were discarded even if it was the best hypothesis.

\section{Future Work} \label{discussion:future_work}
Some suggestions on how to improve current detection method at a high level have already been suggested in section \ref{discussion:weaknesses}.
But, the lower level image processing algorithms can also be changed.
To fix the problem regarding the use of constant thresholds, one can use thresholding techniques to automatically determine good thresholds to detect edges in an image, or a region of an image.
This can be done in conjunction with Canny edge detection, as shown in \cite{wang2005fast} and \cite{liu2004automated}.
Alternatively, a completely different approach could be used for object detection, such as the one presented in "Localizing 3D cuboids in single-view images" as mentioned in section \ref{related_work:object_detection}.

Another interesting improvement is to use multiple images to measure the package.
One could either use multiple measurements and simply calculate an average, combined with outlier rejection with for example the RANSAC algorithm.
This should both improve success rate and accuracy..
In the same way, one could save the intrinsic matrices found with vanishing point calibration between measurements and combine them.
They should then converge over time to a very accurate calibration matrix. \cite{fischler1981random}

% Nämn object tracking och rois.
%\cite{yilmaz2006object}

\section{Conclusions}
TODO

% demo app not pleasant to use in its current state, but it could be if tracking is added, or with a non-realtime solution, or any other combination
% Very difficult to make general detection based on this simple approach









