\chapter{Theoretical Background}

In this chapter the theoretical concepts used throughout the report is presented. Knowledge of two computer vision related fields is necessary to understand the following chapters: projective geometry and image processing.

\section{Projective geometry}
Everyone has a basic understanding of how projective geometry works.
We experience it at every wake moment, without thinking about it. 
The perceived shape of an object can be very different, depending on from where it is viewed.
The square and circle in figure \ref{fig:circle} turn into an ellipse and a trapezium when viewed from a different perspective in figure \ref{fig:circle_perspective}.
The two images are related by a \textit{projective transformation}.
Projective geometry describes these kinds of phenomena.
This section presents some basic concepts in projective geometry, and how, and how images are formed in cameras.

\input{floats/perspective}

As can be seen in figure \ref{fig:perspective}, very little is preserved by a projective transformation.
Neither shape, lengths, angles, distances, or ratios of distances are preserved.
It turns out that the most general property in a scene that is preserved by a projective transformation is straightness. \cite[1]{hartley-zisserman}

\subsection{Homogeneous coordinates}

In euclidean geometry, a point in two-dimensional space is typically represented by cartesian coordinates.
In projective geometry, the homogeneous coordinate system is used instead.
To convert the cartesian coordinate pair $(x,y)$ to homogeneous coordinates, a one is appended to the cartesian coordinates, resulting in the point $(x,y,1)$. 
Conversely, the cartesian representation the homogeneous point $(\tilde{x},\tilde{y},\tilde{z})$ is $(\tilde{x}/\tilde{z},\tilde{y}/\tilde{z})$. 
In other words, homogeneous coordinates ignore scale; $(x,y,1)$ and $(kx,ky,k)$ represents the same point for any nonzero value $k$.
These principles extend to other dimensions as well. \cite[2]{hartley-zisserman}

\subsection{The projective camera Model}\label{camera-model}
Camera create a 3D projection of the world, i.e. a mapping of 3D coordinates to a 2D plane.
Homogeneous coordinates provide an elegant way to perform this mapping, with a model called the projective camera model.
It is based on a pinhole camera model, where all rays of light pass through a single point called the camera centre, $C$. Before reaching $C$, each ray will pierce the image plane, where the image is projected.
It's location is determined by $f$, the focal length of the camera.

\begin{figure}
\begin{center}
\includegraphics[width=1.0\textwidth]{figures/central_projection_camera.png}
\end{center}
\caption{The central projection camera model. This is a simplification of the general projective camera model, where the image plane is placed with offset $f$ on the $Z$ axis but is otherwise aligned with the world coordinate system.}
\label{fig:central_projection_camera} % TODO hur referera? zisserman-hartley s.154
\end{figure}

Using the central projection model, as shown in figure \ref{fig:central_projection_camera}, a 3D point can be mapped to a 2D image point, using this simple mapping: $(X,Y,Z)^T\mapsto(fX/Z,fY/Z)^T$.
The central projection model makes some rather limiting assumptions, which, which we will now generalise.
First, the origin of the image coordinate system is centre not in the centre.
To remedy this, an offset to the principal point, $(p_{x}, p_{y})$, is added, which represents the coordinates of the centre of the image.
Furthermore, the mapping above assumes that the camera is not rotated, and located in the origin of the world coordinate system. Taking this into account, the following equation can be formed:
\begin{equation}\label{eq:projection1}
\begin{pmatrix}	\tilde{u}\\\tilde{v}\\\tilde{w}\end{pmatrix} = 
\begin{pmatrix}
	f & 0 & p_{x} & 0\\
	0 & f & p_{y} & 0\\
	0 & 0 & 1 & 0
\end{pmatrix}
\begin{pmatrix}
	r_{11} & r_{12} & r_{13} & t_{1}\\
	r_{21} & r_{22} & r_{23} & t_{2}\\
	r_{31} & r_{32} & r_{33} & t_{3}
\end{pmatrix}
\begin{pmatrix}	X\\Y\\Z\\1\end{pmatrix}
\end{equation}

where $(\tilde{x},\tilde{y},\tilde{z})^T$ are the homogeneous coordinates of the pixel in the image of the world coordinates $(X,Y,Z)$.
As explained earlier, the cartesian coordinates are obtained by dividing by $\tilde{w}$:
$$
u = \frac{\tilde{u}}{\tilde{w}},~~v = \frac{\tilde{v}}{\tilde{w}}
$$
The above is the projective camera model that will be used in this thesis. 
It can be generalised further to take things like non-square, or skewed pixels, but that is left out, since the shape of pixels is typically very close to square.

The left of the two matrices above is called the intrinsic matrix, $K$.
Its entries, the intrinsic parameters, depend only on the camera, and are always the same for the a given camera, if there is no change in zoom.
The right matrix is called the extrinsic matrix.
It contains the extrinsic parameters of the camera.
The extrinsic parameters are not tied to the camera's properties, but depend on where in the world coordinate system the camera is located, and where it points.
Equation \ref{eq:projection1} can also be written in the more concise form $x = K[R|t]X$.

When multiplied together, the intrinsic and extrinsic matrices form a $3\times4$-matrix, called the camera matrix, $P$.
\begin{equation} \label{eq:projection2}
\begin{pmatrix} \tilde{u} \\ \tilde{v} \\ \tilde{w} \end{pmatrix} =
\begin{pmatrix} p_{11} & p_{12} & p_{13} & p_{14} \\
 				p_{21} & p_{22} & p_{23} & p_{24} \\
				p_{31} & p_{32} & p_{33} & p_{34} \end{pmatrix}
\begin{pmatrix}X \\Y \\Z \\1\end{pmatrix}
\end{equation}
$P$ is a homogeneous matrix, and thus only has 11 degree of freedom.
The above can therefore be written as:
\begin{equation}\label{eq:projection3}
\begin{pmatrix} \tilde{u} \\ \tilde{v} \\ \tilde{w} \end{pmatrix} =
\begin{pmatrix} p_{11} & p_{12} & p_{13} & p_{14} \\
 				p_{21} & p_{22} & p_{23} & p_{24} \\
				p_{31} & p_{32} & p_{33} & 1 \end{pmatrix}
\begin{pmatrix}X \\Y \\Z \\1\end{pmatrix}
\end{equation}
where the scaling of the elements of $P$ is implicit. \cite[153-165]{hartley-zisserman}

\subsection{Planar Homographies}\label{planar-homographies}
Assume that a projective camera is observing a planar scene, and that we would like to map points on the plane to points on the image plane. 
If the world coordinate system is defined so the plane is the plane $Z=0$, the projective camera model can be simplified by removing the $Z$ coordinate and the the third column of $P$ from equation \ref{eq:projection3}. 
This scenario, which is depicted in figure \ref{fig:planar_homography}, yields the following system:
\begin{equation}\label{eq:homography}
\begin{pmatrix} \tilde{u} \\ \tilde{v} \\ \tilde{w} \end{pmatrix} =
\begin{pmatrix} h_{11} & h_{12} & h_{13}  \\
 				h_{21} & h_{22} & h_{23}  \\
				h_{31} & h_{32} & h_{33}\end{pmatrix}
\begin{pmatrix}X \\Y \\ 1\end{pmatrix}
\end{equation}

\begin{figure}
\begin{center}
\includegraphics[width=1.0\textwidth]{figures/planar_homography.pdf}
\end{center}
\caption{A point on a plane in the world is projected to the image plane. A planar homography can be used to map points between the two planes.} % TODO hur referera? Tagen från hartley-zisserman s. 34
\label{fig:planar_homography}
\end{figure}

This transformation is called a planar homography. The homography matrix, $H$ has 8 degrees of freedom. 
This is a linear equation system, meaning that $H$ can be determined with 8 linear equations.
A pair of known corresponding points on the two planes, $x=HX$ will each contribute with two linear equations to constrain $H$.
Thus, four known corresponding points are required to fully determine $H$.
This assumes that that each point contributes with a unique equation.
If three or more of the points in a minimal solution are collinear, a unique solution cannot be determined.
Such a configuration is \textit{degenerate}. \cite[91-92]{hartley-zisserman}

Since the positions of the corresponding points are typically noisy, an exact solution can often not be found.
One method to find a solution with minimal error is the Direct Linear Transformation algorithm (DLT). \cite{homography-estimation}

\subsection{The Plane at Infinity and the Absolute Conic} \label{ac}
Two critical concepts in projective geometry are the plane at infinity, $\pi_{\infty}$ and the absolute conic, $\omega_{\infty}$.
These are very theoretical concepts and will only be touched upon briefly due to their importance for camera calibration.

As mentioned earlier, parallelism is not preserved by projective transformations. % förklarat begreppet projective transformations tidigare?
As a consequence, lines that are parallel in euclidean space are not necessarily parallel in projective space, but intersect at a vanishing point.
An example of this is how parallel train tracks meet at the horizon.

$\pi_{\infty}$ is a plane where parallel lines meet. 
On $\pi_{\infty}$ lies an imaginary conic, the absolute conic, $\omega_{\infty}$.
Its projection onto the image plane is called the Image of the Absolute Conic.
The full theory regarding these concepts is out of scope for this thesis, but the important thing to know is that $\omega_{\infty}$ is invariant to projective transformations.
This has important implications for camera calibration because, because it means that $\omega_{\infty}$ acts as a natural calibration object present in every image.
It can be shown that the following relationship exists between $K$ and the image of the image of the absolute conic:
$\omega = (KK^T)^{-1}$. \cite[210]{hartley-zisserman}\cite{pollefeys}

\subsection{Camera Calibration} \label{camera-calibration}
This section will describe how to determine the camera matrix $P$, and its components.
This problem is known as camera calibration.
It is a similar problem to the one described in \ref{planar-homographies}, and can be solved in the same manner.

The difference from the 2D case is that matrix has an extra column. The number of unknowns is now eleven instead of eight, which means that five and a half point correspondences (meaning five points and one $x$ or $y$ correspondence) are needed instead of four.
The problem of degeneracy is also more complicated than in the 2D case, since we try to make projections in three dimensions instead of two.
The most important case is that degeneracy occurs if all point known correspondances lie in the union of a plane and a line. \cite[179-180]{hartley-zisserman}

A classic strategy for calibrating cameras is to use orthogonal planes with a known pattern on them. 
Since the planes are orthogonal, degeneracy is avoided.

A more flexible, and now very popular method, was presented by Zhengyou Zhang in his paper "Flexible Camera Calibration By Viewing a Plane From Unknown Orientations".
Zhang's method uses multiple images (at least two) from different perspectives of a single known, planar pattern.
Degeneracy is avoided by using multiple images, and since many point correspondences are known in each image, the solution becomes very over-determined, which improves accuracy. \cite{zhang-calibration}

Since $P=K[R|t]$, and $K$ remains the same between images taken by the same camera, $K$ can be extracted from $P$ through QR-decomposition, and reused later even if the camera has moved \cite{wiki:qr-decomposition}. 
This is desirable to do, because if $K$ is known, the extrinsic parameters can be determined using less information than what is needed to determine $P$ from scratch, as described in section \ref{camera-pose}.

\subsubsection{Camera calibration using Vanishing Points}
It is also possible to determine $K$ is to do so without first determining $P$, for example by using vanishing points.
One way to do that is by using vanishing points.
It turns out that $K$ can be determined from a single image by imposing constraints on the image of the absolute conic ($\omega$) and then using the equation from section \ref{ac}:
\begin{equation} \label{eq:conic_k}
	\omega = (KK^T)^{-1}
\end{equation}

Like $K$, $\omega$ is represented by a symmetric, homogeneous matrix with the following structure
$$
\omega = \begin{pmatrix}
	w_{1} & w_{2} & w_{4} \\
	w_{2} & w_{3} & w_{5} \\
	w_{4} & w_{5} & w_{6} 
\end{pmatrix}
$$
which can be derived the fact that the structure of $K$ is known, and equation \ref{eq:conic_k}.

Three types of linear constraints that can be imposed on $\omega$:
\begin{enumerate}
	\item metric information from a plane in the image with a known homography
	\item information from orthogonal vanishing points
	\item internal constraints from $K$, like no skew or square pixels.
\end{enumerate}

We will begin with the latter type.
If it is known that pixels are square, as is assumed in this thesis, then $\omega$ has the following form:
$$
\omega = \begin{pmatrix}
	w_{1} & 0 & w_{2} \\
	0 & w_{1} & w_{3} \\
	w_{2} & w_{3} & w_{4}
\end{pmatrix}
$$
Now, only three equations are needed, since $\omega$ is a homogeneous matrix.

Orthogonal vanishing points give rise to constraints of the form $v_{1}^T \omega v_{2} = 0$.
A known homography $H=[h_{1},h_{2},h_{3}]$ give the two constraints $h_{1}^T \omega h_{2} = 0$ and $h_{1}^T \omega h_{1} = h_{2}^T \omega h_{2}$. Refer to \cite{hartley-zisserman} chapter 8 for an explanation on how these equations are derived.

Each constraint can be rewritten to the form $a_{1 \times 4}$w$_{4 \times 1}=0$ where w contains the four elements of $\omega$.

The $n$ equations are then stacked on top of each to form the system $A_{n \times 4}$w$=0_{4 \times 1}$
This system can be solved using SVD to determine $\omega$.
Finally, $\omega$ is decomposed into $K$ by matrix inversion followed by Cholesky factorization. \cite[223-226]{hartley-zisserman}

\subsubsection{Obtaining Camera Pose with known $K$} \label{camera-pose}
When $K$ is known, the final step before points can be projected using equation \ref{eq:projection1}, is to determine the pose of the camera, i.e. extrinsic matrix, $[R|t]$. $[R|t]$ represents the rotation and translation of the camera in the world coordinate frame, where $R$ is a $3 \times 3$ matrix and $t$ is a $3 \times 1$ vector.
Constraints can be imposed through point correspondences like in previous cases.
The $R$ and $t$ have three degrees of freedom each, which means six constraints are required to determine them.
As a result, three point correspondences are required to determine the pose, since each correspondence generates two constraints.
But, the resulting system is non-linear, so it is more difficult to solve than in the earlier cases. \cite[187]{hartley-zisserman}

However, there exist a large number of techniques to solve this problem which is known as Perspective-n-Point (PnP). % TODO improve this section
Both iterative, e.g. \cite{hesch-pnp} and \cite{oberkampf-pnp}, and non-iterative solutions exist, e.g. \cite{quan-pnp} and \cite{lepetit-pnp}.
The latter method is called EPnP, and claims to be both faster and more accurate than other methods. 

\section{Image Processing}
Image processing concerns processing of digital images, which consist of small image elements, pixels.
A distinction is often made between the field of image processing and image analysis, where image processing refers to low-level processing, and image analysis concerns high-level processing.
Both the input and output of an image processing algorithm are typically images, for example algorithms for noise, contrast enhancement, and colour correction. 
In contrast, the input of an image analysis algorithm is typically an iamge, but the output is some representation of features in the image.
The task could example be image segmentation, i.e. the task of dividing an image into regions or objects.\cite[p. 1-2]{pitas}\cite[p. 1-2]{gonzalez-woods}
In this thesis the two concepts will be used interchangeably henceforth.
TODO What to write? How detailed?% TODO write

















